## Introduction

A music streaming startup, Sparkify, 
has grown their user base and song 
database and want to move their processes 
and data onto the cloud. Their data resides in S3, 
in a directory of JSON logs on user activity on the app, 
as well as a directory with JSON metadata on the songs 
in their app.

The purpose of this project is to:
1. Create an AWS Redshift Cluster
2. Build an ETL pipeline to move data from S3 buckets onto 
Redshift staged tables
3. Transform staged data info an dimensional model

## Project dataset

There are 2 datasets provided for this project:
* Song dataset
* Log dataset

### Song dataset

The first dataset is a subset of real data from the 
Million Song Dataset. Each file is in JSON format and 
contains metadata about a song and the artist of that 
song. The files are partitioned by the first three 
letters of each song's track ID.

Record example:

`{
    "num_songs": 1, 
    "artist_id": "ARJIE2Y1187B994AB7", 
    "artist_latitude": null, 
    "artist_longitude": null, 
    "artist_location": "", 
    "artist_name": "Line Renaud", 
    "song_id": "SOUPIRU12A6D4FA1E1", 
    "title": "Der Kleine Dompfaff", 
    "duration": 152.92036, 
    "year": 0
}`

### Log Dataset

The second dataset consists of log files in JSON format 
generated by an event simulator based on the songs in 
the dataset above. These simulate app activity logs from
an imaginary music streaming app based on configuration 
settings. The log files in the dataset are partitioned by year 
and month.

Record example:

`{
    "artist": "Pavement", 
    "auth": "Logged In", 
    "firstName": "Sylvie", 
    "gender": "F", 
    "itemInSession": 0, 
    "lastName": "Cruz", 
    "length": 99.16036, 
    "level": "free", 
    "location": "Washington-Arlington-Alexandria, DC-VA-MD-WV", 
    "method": "PUT",
    "page": "NextSong",
    "registration": 1.540226e+12,
    "sessionId": 345,
    "song": "Mercy:The Laundromat",
    "status": 200,
    "ts": 1541990258796,
    "userAgent": "Mozilla/5.0 (Macintosh;Intel Mac OS X)",
    "userId": 10
}`

## Target data model

Using the dataset provided, the goal of this project is to create
a dimensional data model, specifically a star schema, consisting of the
following tables:

### Fact table 

**songplays** - records in event data associated with song plays i.e. 
records with page 'NextSong'

### Dimension tables

* **users** - characteristics of users in the app
* **songs** - all songs in music database
* **artists** - all artists in music database
* **time** - timestamps of records in songplays broken 
down into specific units

## Instructions

1. Provide AWS key and secret of a user with 
_AdministratorAccess_ role in the _aws_auth.cfg_ file.
2. Run _create_redshift_cluster.py_ script to create and
initiate a Redshift cluster. The script will also write
DWH endpoint and role arn into _dwh.cfg_ file.
3. Run _create_tables.py_ file to create staging and target
tables.
4. Run _etl.py_ to insert data into previously created
tables.
5. It will take circa 50 minutes in order to load all 
the data.
6. Run _delete_redshift_cluster.py_ script to delete
Redshift cluster.